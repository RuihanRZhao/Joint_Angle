
Our pose estimation network is built around a lightweight Cross Stage Partial (CSP) backbone enhanced with channel attention and a streamlined fusion head. In this section we describe the basic building blocks, the three backbone stages, the multi‐scale feature fusion strategy, and the SimCC prediction head in detail.

\paragraph{Basic Building Blocks}
\begin{itemize}
  \item \textbf{ConvBNAct:} A composite layer comprising a $k\times k$ convolution, Batch Normalization, and SiLU activation.  Both kernel size and padding are chosen to preserve spatial dimensions.  (See \texttt{ConvBNAct} in \texttt{Joint\_Pose.py}.)
  \item \textbf{Residual Unit:} A two‐layer sequence of $3\times3$ ConvBNAct modules (the second without activation) whose output is added back to the input, followed by a final SiLU.  This design facilitates gradient flow and stability.
  \item \textbf{SEBlock:} A Squeeze‐and‐Excitation module that performs global average pooling to produce a $C\times1\times1$ descriptor, followed by a two‐layer fully connected bottleneck (reduction ratio $r=16$) and a sigmoid gating to recalibrate channel responses.
  \item \textbf{CSPBlock:} Given an input tensor $X\in\mathbb{R}^{B\times C_{\mathrm{in}}\times H\times W}$, it is first split via two $1\times1$ ConvBNAct paths:
    \[
      Y_1 = \mathrm{ConvBNAct}_{1\times1}(X), \quad
      Y_2 = \mathrm{ConvBNAct}_{1\times1}(X).
    \]
    Path $Y_1$ passes through $n$ Residual Units, while $Y_2$ bypasses them.  The two outputs are concatenated along the channel axis, fused by another $1\times1$ ConvBNAct, and finally passed through an SEBlock:
    \[
      \mathrm{CSPBlock}(X) = \mathrm{SEBlock}\bigl(\mathrm{ConvBNAct}_{1\times1}\bigl[\mathrm{Concat}(Y_1, Y_2)\bigr]\bigr).
    \]
\end{itemize}

\paragraph{Backbone Stages}
Input images of size $H\times W$ (default $384\times384$) are first downsampled by the \emph{Stem} layer:
\[
  \text{Stem} :\; 3\times3\ \text{ConvBNAct},\ \text{stride}=2,\ C=64
\quad\Longrightarrow\quad \tfrac{H}{2}\times\tfrac{W}{2}\times64.
\]
This is followed by three sequential stages:
\begin{enumerate}
  \item \textbf{Stage 1:}
    \begin{itemize}
      \item $3\times3$ ConvBNAct (stride 2) to $128$ channels $\;\Rightarrow\;\tfrac{H}{4}\times\tfrac{W}{4}\times128$.
      \item CSPBlock($C_{\mathrm{in}}=128,\;C_{\mathrm{out}}=128,\;n=2$) + SEBlock.
    \end{itemize}
  \item \textbf{Stage 2:}
    \begin{itemize}
      \item $3\times3$ ConvBNAct (stride 2) to $256$ channels $\;\Rightarrow\;\tfrac{H}{8}\times\tfrac{W}{8}\times256$.
      \item CSPBlock($256\!\to\!256,\;n=4$) + SEBlock.
    \end{itemize}
  \item \textbf{Stage 3:}
    \begin{itemize}
      \item $3\times3$ ConvBNAct (stride 2) to $512$ channels $\;\Rightarrow\;\tfrac{H}{16}\times\tfrac{W}{16}\times512$.
      \item CSPBlock($512\!\to\!512,\;n=4$) + SEBlock.
    \end{itemize}
\end{enumerate}

\paragraph{Multi‐Scale Feature Fusion}
To leverage both high‐level context and fine details, we fuse features from all three stages in a top‐down manner:
\[
\begin{aligned}
  F_3 &= \text{Stage3\_output}\quad (B\times512\times\tfrac{H}{16}\times\tfrac{W}{16}),\\
  F_3' &= \mathrm{ConvBNAct}_{1\times1}(F_3)\quad(512\!\to\!256),\\
  \widetilde F_2 &= \mathrm{Upsample}(F_3',\; \text{scale}=2) + F_2,\\
  F_2' &= \mathrm{ConvBNAct}_{3\times3}(\widetilde F_2),\\
  \widetilde F_1 &= \mathrm{Upsample}\bigl(\mathrm{ConvBNAct}_{1\times1}(F_2'),\; \text{scale}=2\bigr) + F_1,\\
  F_1' &= \mathrm{ConvBNAct}_{3\times3}(\widetilde F_1),
\end{aligned}
\]
where $F_2,F_1$ are the raw outputs of Stage 2 and Stage 1 respectively.  The result is $F_1'\in\mathbb{R}^{B\times128\times\frac{H}{4}\times\frac{W}{4}}$.

\paragraph{SimCC Prediction Head}
From $F_1'$, we predict each of the $K$ keypoints’ horizontal and vertical distributions via two parallel convolutions:
\[
  X_{\text{logits}} = \mathrm{Conv2d}\bigl(128\!\to\!K,\;(H/4,1)\bigr),\quad
  Y_{\text{logits}} = \mathrm{Conv2d}\bigl(128\!\to\!K,\;(1,W/4)\bigr).
\]
Squeezing the singleton dimensions yields tensors of shape $(B,K,W/4)$ and $(B,K,H/4)$, which are then upsampled by a factor of $\zeta$ (the per‐pixel bin count, typically $4$) using depthwise transposed convolutions.  Taking the argmax along each axis gives sub‐pixel coordinate estimates without requiring large 2D heatmaps.

\paragraph{Model Complexity}
The full network contains approximately $2.9$ million parameters and requires about $3.5$ GFLOPs per inference on $384\times384$ inputs.  On an NVIDIA H200 GPU, this design achieves over 60 FPS in practice, making it well‐suited for real‐time edge deployment.  
