The model was trained for 200 epochs with batch size 512 using the AdamW optimizer (learning rate $5\times10^{-3}$, weight decay $1\times10^{-4}$) and a One-Cycle learning rate schedule (10\% warm-up, cosine annealing, restarts every 60 epochs). Automatic Mixed Precision (AMP) was enabled to accelerate training. Experiments were logged with Weights \& Biases, tracking training loss components and validation metrics (mAP, AP@50) each epoch.
