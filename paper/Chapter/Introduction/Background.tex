Human pose estimation (HPE) aims to locate a set of anatomical keypoints (e.g., joints) on the human body from images or video. Early methods treated this as a graphical model problem, using hand-crafted features and part-based models. A seminal approach was the pictorial structures framework, which represents each limb as a deformable part connected in a tree and infers joint locations by optimizing pairwise spatial relationships \cite{Felzenszwalb2005}. While effective in controlled settings, these classical methods struggled with the high variability of real-world poses, occlusions, and complex backgrounds.

The introduction of deep learning dramatically advanced HPE accuracy. Toshev and Szegedy \cite{Toshev2014DeepPose} proposed DeepPose, the first end-to-end convolutional network to directly regress joint coordinates. However, direct coordinate regression proved challenging to train for high precision. Subsequent works reframed pose estimation as dense heatmap prediction for each keypoint, yielding substantial gains in localization accuracy \cite{Tompson2015}. Fully convolutional architectures such as the Stacked Hourglass network \cite{Newell2016} further improved performance by iteratively refining predictions over multiple stages. More recently, High-Resolution Networks (HRNet) maintained high-resolution feature maps throughout the network and achieved state-of-the-art accuracy by repeatedly fusing multi-scale information \cite{Sun_2019_CVPR}. These deep models now surpass traditional approaches by wide margins on benchmarks such as MPII and COCO \cite{Lan2023}.

Building on the success of convolutional networks, researchers have also explored transformer-based architectures for pose estimation. By leveraging self-attention mechanisms, transformer models can capture long-range dependencies between body joints. For example, TransPose and follow-on works integrate global context and have demonstrated competitive accuracy while simplifying the grouping of multi-person poses \cite{Stoffl2021}. Although transformers further close the accuracy gap, they often introduce additional computational overhead, rendering them less suitable for resource-constrained devices.

Extending to multi-person scenarios, HPE methods follow either a top-down or bottom-up paradigm. Top-down methods first detect each person with a general object detector (e.g., Mask R-CNN) and then apply a single-person pose estimator to each crop \cite{He2017}. This yields high per-person accuracy, but runtime grows linearly with the number of people. Bottom-up methods detect all keypoints for all people in a single pass and then assemble them into individual poses \cite{Cao_2017_CVPR}. Bottom-up pipelines have runtime largely independent of the number of people, making them attractive for crowded or real-time settings, though they often lag slightly behind top-down methods in raw accuracy \cite{Dubey2023PoseSurvey}. Hybrid approaches balance these trade-offs by learning grouping as part of the network.

Despite the remarkable accuracy of modern deep learning models, most state-of-the-art HPE networks are extremely large and computationally expensive. For instance, HRNet-W48 contains over 60 million parameters, and OpenPose’s two-branch network requires tens of GFLOPs per image. Such complexity hinders deployment on edge devices (e.g., mobile phones, embedded cameras) where memory, compute, and power budgets are strictly limited. Applications such as mobile fitness tracking, augmented reality, and human–robot interaction demand on-device pose estimation to ensure low latency, data privacy, and offline operation. Consequently, there is a pressing need for methods that deliver competitive pose estimation accuracy while operating within tight edge-device constraints.

This thesis investigates one such approach: combining an efficient convolutional backbone (CSPNet), compact attention modules (SE blocks), and a streamlined coordinate-based output representation (SimCC) to design a lightweight, edge-ready human pose estimator.
