This thesis attempted to build a tiny human pose estimator using a CSPNet backbone with SE attention and SimCC regression, trained from scratch on a COCO single-person subset. Despite extensive effort, the model failed to generalize—its validation AP remained near 0\%. The failure resulted from a combination of insufficient data, lack of transfer learning, unoptimized SimCC parameters, and an underpowered architecture.

In reflecting on these negative outcomes, several key conclusions emerge. First, the absence of ImageNet pretraining deprived the network of foundational filters and hierarchical feature representations, reinforcing the critical role of transfer learning in building robust lightweight models \cite{Mathis2021Pretraining}. Second, the limited diversity and volume of our training set demonstrated that model expressivity must be matched by data richness; without varied poses, scales, and backgrounds, even well‐designed architectures overfit rapidly \cite{Dubey2023PoseSurvey}. Third, the extreme sensitivity of the SimCC head to hyperparameters—particularly spatial bin count and label smoothing—underscored the need for systematic parameter optimization, ideally via automated search frameworks. Finally, our one‐stage design lacked the multi‐level refinement and teacher‐student guidance shown to boost small‐model performance by up to 6\% mAP \cite{Li2021OKD}.

Moving forward, we propose a comprehensive roadmap that integrates these lessons: employing pretrained backbones with progressive layer unfreezing, expanding datasets through external sources and advanced augmentation, automating SimCC tuning, and incorporating knowledge distillation. If these strategies are applied effectively, we anticipate raising COCO AP into the mid‐60s while maintaining a sub‐5M parameter count. Ultimately, this work illustrates that negative experimental results are not failures but valuable waypoints, guiding the development of resilient, edge‐ready pose estimators.
