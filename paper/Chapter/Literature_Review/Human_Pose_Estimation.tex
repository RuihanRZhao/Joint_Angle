Human pose estimation (HPE) is the task of predicting human joint locations from images. Classical approaches relied on pictorial structures and graphical models, using hand-crafted features to model body parts and their spatial constraints \citep{Felzenszwalb2005}. These methods often employed tree-structured models or pictorial springs to represent kinematic relationships, but they were limited by simplistic appearance models and computational complexity. The deep learning era brought significant breakthroughs: early convolutional models like DeepPose directly regressed joint coordinates \citep{Toshev2014}, while later methods learned heatmap representations for each keypoint, greatly improving accuracy \citep{Tompson2015}. Fully convolutional networks such as the Stacked Hourglass network \citep{Newell2016} and others achieved further gains by refining predictions over multiple stages. Modern HPE models overwhelmingly use deep CNN or transformer backbones to learn rich feature representations of the human body.

A central paradigm in HPE is the distinction between top-down and bottom-up strategies for multi-person pose estimation. Top-down methods first detect individual people in the image (e.g. via a person detector) and then perform single-person pose estimation on each cropped region \citep{He2017}. This approach can leverage powerful single-person pose networks (often yielding high accuracy for each person), but its runtime grows with the number of people. In contrast, bottom-up methods detect all body keypoints in the image first, and then group keypoints into individuals \citep{Cao2017}. Bottom-up approaches (exemplified by OpenPose \citep{Cao2017}) are more efficient for images with many people, as the feature extraction is shared, but they face the challenge of correctly associating keypoints into distinct persons. Both paradigms have achieved success, and hybrid approaches also exist (e.g. using learned grouping or graph models), with trade-offs between accuracy and efficiency.

Recent trends push toward more end-to-end learning and integrated frameworks in pose estimation. Instead of relying on separate detection and post-processing steps, newer models aim to predict poses directly from the image in a single pass. For example, one-stage architectures have been proposed that simultaneously localize people and their keypoints within one network \citep{Nie2019}. Transformer-based pose models further integrate global context and can output multi-person poses without explicit grouping algorithms, treating the problem as a direct set prediction \citep{Stoffl2021}. These end-to-end approaches simplify the overall pipeline and are attractive for real-time applications, although they must still balance the classic precision-efficiency trade-offs.
