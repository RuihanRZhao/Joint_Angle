Human pose estimation (HPE) seeks to localize a set of predefined anatomical keypoints—such as joints of the limbs, torso, and head—in 2D images. Early work in this field relied on part-based graphical models, most notably the pictorial structures framework \citep{Felzenszwalb2005}, which represents each body part by a deformable template and enforces kinematic constraints via a tree-structured model. Although these methods could capture simple pose variations, they were hampered by limited appearance modeling (relying on hand-crafted features like SIFT or HOG) and expensive inference due to the combinatorial nature of part configurations.

The advent of deep convolutional neural networks (CNNs) marked a paradigm shift. Toshev and Szegedy \citep{Toshev2014DeepPose} introduced DeepPose, an end-to-end network that directly regressed joint coordinates from input images, demonstrating the feasibility of deep learning for pose tasks. However, direct regression struggled to achieve fine localization accuracy. Subsequently, Tompson \emph{et al.} \citep{Tompson2015} proposed predicting a probability heatmap for each keypoint, converting coordinate estimation into a dense per-pixel classification problem. This formulation preserved spatial context and yielded substantial accuracy improvements.

Building on heatmap representations, iterative refinement architectures emerged. The Stacked Hourglass network \citep{Newell2016} uses repeated down- and up-sampling modules with intermediate supervision to progressively refine pose estimates. Later, Residual Pose Machines and Convolutional Pose Machines extended this idea with multi-stage feature fusion. A major advance came with High-Resolution Networks (HRNet) \citep{Sun_2019_CVPR}, which maintain high-resolution features throughout all stages and fuse multi-scale information in parallel, setting new state-of-the-art results on benchmarks like MPII and COCO.

For multi-person scenarios, two principal paradigms have been developed. Top-down methods first detect individual persons via a general detector (e.g., Mask R-CNN \citep{He2017}) and then apply a single-person pose estimator to each crop, achieving high per-person accuracy but incurring additional cost per person. Bottom-up methods, exemplified by OpenPose \citep{Cao_2017_CVPR}, detect all body keypoints in a single forward pass and then group them into person instances, yielding runtime largely independent of population size but posing challenges in grouping accuracy.

Recent research trends include one-stage detectors that jointly perform person localization and keypoint regression in a single network \citep{Nie2019}, and transformer-based architectures that model global context and output sets of poses via attention mechanisms \citep{Stoffl2021}. These approaches streamline the pipeline by eliminating separate detection or grouping steps, yet must still manage the trade-off between precision and computational efficiency.

Overall, the evolution from pictorial structures to deep heatmap regression—and now to coordinate classification and transformers—reflects a continual push for higher accuracy and simpler pipelines. However, the most accurate models remain computationally intensive, motivating the development of lightweight and compressed architectures for real-time, on-device applications.