We present a novel end-to-end lightweight model for simultaneous human parsing (semantic part segmentation) and pose estimation, distilled from a two-stage teacher model. The teacher consists of a heavyweight segmentation network and a pose estimation network. Our student network shares a compact backbone with two task-specific heads. During training, we employ multi-level knowledge distillation: the student mimics the teacherâ€™s dense predictions and feature representations. This allows the student to learn rich semantic and structural information without the computational cost of separate large models. We derive rigorous formulations for the distillation losses, including pixel-wise KL divergence for segmentation and heatmap regression for pose. Experiments on COCO and MPII benchmarks demonstrate that our distilled student achieves accuracy comparable to state-of-the-art methods while being orders of magnitude smaller. In particular, we outperform existing efficient models such as Lite-HRNet and TinyPose in pose accuracy, and maintain strong segmentation quality. Ablation studies confirm that each distillation component contributes significantly. Our method offers a practical solution for resource-constrained human analysis tasks.