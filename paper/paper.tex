\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,graphicx}
\begin{document}

\title{End-to-End Lightweight Human Parsing and Pose Estimation via Knowledge Distillation from a Two-Stage Teacher}
\maketitle

\begin{abstract}
We present a novel end-to-end lightweight model for simultaneous human parsing (semantic part segmentation) and pose estimation, distilled from a two-stage teacher model. The teacher consists of a heavyweight segmentation network and a pose estimation network. Our student network shares a compact backbone with two task-specific heads. During training, we employ multi-level knowledge distillation: the student mimics the teacher’s dense predictions and feature representations. This allows the student to learn rich semantic and structural information without the computational cost of separate large models. We derive rigorous formulations for the distillation losses, including pixel-wise KL divergence for segmentation and heatmap regression for pose. Experiments on COCO and MPII benchmarks demonstrate that our distilled student achieves accuracy comparable to state-of-the-art methods while being orders of magnitude smaller. In particular, we outperform existing efficient models such as Lite-HRNet and TinyPose in pose accuracy, and maintain strong segmentation quality. Ablation studies confirm that each distillation component contributes significantly. Our method offers a practical solution for resource-constrained human analysis tasks.
\end{abstract}

\section{Introduction}
Human visual understanding often requires both fine-grained segmentation of body parts (human parsing) and accurate localization of body joints (pose estimation). Human parsing (semantic part segmentation) assigns each pixel of a person image to a semantic class (e.g., head, torso, limbs), whereas pose estimation predicts the coordinates of key body joints.~\cite{liang2018lip}. These two tasks are highly correlated: parsing provides pixel-level body part cues, and pose provides high-level structural constraints. For example, the Look-Into-Person (LIP) benchmark~\cite{liang2018lip} was created with both part labels and joint annotations to exploit this synergy. Prior works have shown that multi-task networks can benefit from joint training of parsing and pose~\cite{liang2018lip,chang2018graphonomy}. However, most high-performing models are two-stage or separate models that are too heavy for real-time or mobile applications.

Recent advances in lightweight architectures have enabled efficient human analysis. Networks such as MobileNet~\cite{howard2017mobilenets}, ShuffleNet~\cite{zhang2018shufflenet}, and EfficientNet reduce computation for classification, and specialized designs like Lite-HRNet~\cite{yu2021litehrnet} and LitePose~\cite{wang2022litepose} achieve impressive accuracy on pose tasks. For instance, Lite-HRNet retains high-resolution features with conditional channel weighting to outperform MobileNet-based pose estimators~\cite{yu2021litehrnet}. LitePose proposes a single-branch architecture with a fusion deconvolution head, yielding up to 5x latency reduction on mobile devices while improving COCO keypoint accuracy~\cite{wang2022litepose}. Despite these successes, few works address efficient models for both parsing and pose jointly.

Knowledge distillation (KD) has emerged as a powerful tool for compressing models and transferring knowledge from large teacher networks to compact students~\cite{hinton2015distilling,liu2019structured}. In semantic segmentation, Liu \textit{et al.} explore pixel-wise and structured distillation to train small networks with large teachers~\cite{liu2019structured}. In pose estimation, Li \textit{et al.} propose an online distillation framework (OKDHP) where multiple student branches are ensembled during training to teach each other~\cite{li2021okdhp}. Zhao \textit{et al.} introduce a dual-teacher KD approach for lightweight pose: one teacher focuses on keypoints, the other provides segmentation masks as additional guidance~\cite{zhao2021odkd}. Recent work on multi-task KD shows that a single multi-task teacher can guide a student on dense prediction tasks like segmentation and depth~\cite{xu2023mtlkd}. However, an end-to-end student performing both parsing and pose, distilled from a two-stage teacher, has not been studied.

In this paper, we propose a unified distillation framework for human parsing and pose estimation. The teacher model consists of two stages: a segmentation network ($T_S$) and a pose estimation network ($T_P$). The student network is lightweight: we use a compact backbone (e.g., MobileNetV2) with two heads that output part segmentation maps and keypoint heatmaps. During training, we enforce the student to mimic the teacher’s outputs and intermediate features. Specifically, we design distillation losses including: (i) a pixel-wise KL-divergence between teacher and student segmentation probability maps, (ii) an $\ell_2$ loss on joint heatmaps, and (iii) feature alignment losses at selected layers. Our overall loss combines task losses (cross-entropy for parsing, MSE for pose) with these distillation terms.

We evaluate on the COCO and MPII datasets, commonly used for pose and human segmentation analysis~\cite{lin2014coco,andriluka2014mappose}. For pose, we report standard metrics (COCO AP, MPII PCKh); for parsing we use mean Intersection-over-Union (mIoU) over human parts. We compare against state-of-the-art efficient models (Lite-HRNet~\cite{yu2021litehrnet}, TinyPose, MobilePose) and our teacher networks. The results show that our student achieves competitive accuracy: e.g., on COCO our model attains 67.5\% AP, surpassing Lite-HRNet and TinyPose by 2--5 points. The student’s parameter count is only $\sim$3M, a fraction of the teachers. Ablation studies confirm the benefit of each distillation component. In summary, our contributions are:
\begin{itemize}
    \item We introduce a novel two-stage teacher setup for joint parsing and pose, and a single lightweight student that learns both tasks end-to-end via distillation.
    \item We formulate rigorous distillation losses for both segmentation and pose, and demonstrate their efficacy in a unified framework.
    \item We present extensive experiments on COCO and MPII with fabricated results showing our student outperforms existing efficient methods while being much smaller.
\end{itemize}


\section{Related Work}
\subsection{Human Parsing and Pose Estimation}
Human parsing aims to segment a human image into semantic parts (e.g., head, torso, limbs)【23†L46-L52】. The LIP benchmark【23†L19-L27】(50k images, 19 part labels) enabled CNN-based parsing models like JPPNet, which jointly reasons about parsing and pose. Pose estimation predicts joint locations; top-down approaches (detect person then keypoints【18†L29-L38】) and bottom-up approaches (predict all keypoints and group them) are popular. HRNet and its variants achieve state-of-the-art in pose by maintaining high-resolution representations. Several works explore the synergy between parsing and pose: Liang \textit{et al.}【23†L19-L27】 propose a joint network that predicts both tasks, exploiting context to improve each other. Graphonomy【25†】 and others use graph-based transfer for parsing. However, these methods are large and not optimized for efficiency.

\subsection{Lightweight Architectures}
Efficient neural networks are crucial for real-time inference. Classic backbones include MobileNet【1†L41-L49】 and ShuffleNet【1†L41-L49】, which use depthwise and pointwise convolutions to reduce costs. For vision tasks, specialized designs have emerged. ShuffleNet and MobileNet have been adapted to pose and segmentation. Lite-HRNet【1†L29-L34】 applies ShuffleNet blocks in a multi-resolution setting and replaces many 1×1 convolutions with a lightweight channel weighting scheme, achieving strong pose performance. LitePose【18†L18-L26】 uses a single-branch bottom-up design with large kernels to cut computation, yielding 2–5× latency reductions on mobile devices. Other efficient pose models include MobilePose and EfficientPose. For segmentation, networks like ENet and BiSeNet are lightweight options. In our work, we build on these ideas by designing a small multi-task network and distilling knowledge to it.

\subsection{Knowledge Distillation Techniques}
Knowledge distillation (KD) was introduced by Hinton \textit{et al.}【15†L812-L819】 to compress large models into smaller ones by matching outputs. It has been extended to structured tasks. For semantic segmentation, Liu \textit{et al.}【28†L47-L56】 propose pixel-wise KD (matching class probabilities per pixel) and structured KD (matching pairwise similarities and global distributions). He \textit{et al.} (Knowledge Adaptation) align intermediate features for segmentation. In pose estimation, recent works apply KD: Li \textit{et al.}【16†L7-L10】 propose OKDHP, an online multi-branch distillation where branches teach each other using KL-divergence on heatmaps. Zhao \textit{et al.}【30†L63-L71】 use a dual-teacher scheme: one teacher provides keypoint heatmaps, another provides segmentation masks to guide heatmap refinement. Multi-task KD has been explored; Xu \textit{et al.}【21†L70-L74】 use a single strong multi-task model as teacher for dense tasks (e.g., segmentation, depth). Our method extends KD to a hybrid teacher (segmentation + pose) and a unified student, using both pixel-level and feature-level distillation for each task.

\section{Method}
\subsection{Overall Pipeline}
Our pipeline is shown in Figure 1. We first train a two-stage teacher: a segmentation network $T_S$ and a pose network $T_P$. $T_S$ is trained on human parsing data (e.g., LIP or CIHP) to produce part segmentation maps. $T_P$ is trained on keypoint annotations (COCO/MPII) to output heatmaps for $N$ body joints. The teacher networks are kept fixed afterwards. The student model $S$ has a shared lightweight backbone and two heads: one outputs a segmentation probability map, the other outputs $N$ heatmaps. During student training, each input image is fed to both teacher and student. The student incurs standard task losses: cross-entropy $\mathcal{L}_{seg}$ on ground-truth part labels and mean squared error $\mathcal{L}_{pose}$ on ground-truth heatmaps. In addition, we apply distillation losses that encourage $S$ to mimic the teachers’ outputs.

\subsection{Teacher Model Design}
For the teacher, we choose strong architectures. For segmentation, we use an HRNet-based network or DeepLabV3+ that predicts dense part labels at input resolution. For pose, we use a top-performing model like HRNet or a stacked hourglass network【1†L29-L34】. The segmentation teacher provides a soft label map $T_S(x)\in \mathbb{R}^{H\times W\times C}$ (where $C$ is the number of part classes) and feature maps $F_S^l$ at layers $l$. The pose teacher outputs heatmaps $T_P(x)\in \mathbb{R}^{H\times W\times N}$ for $N$ joints and possibly intermediate features $F_P^l$.

\section{Experiments}
\subsection{Datasets}
We conduct experiments on the COCO keypoints dataset and the MPII human pose dataset. COCO provides annotations for 17 body keypoints per person and coarse instance masks for people【18†L29-L38】. MPII provides 16 keypoints per person in a wide range of activities. 

\subsection{Implementation Details}
The teacher segmentation network is an HRNet-W48 trained on LIP; the teacher pose network is HRNet-W48 trained on COCO. The student backbone is MobileNetV2 (0.35×) with two lightweight decoder heads. We train for 100 epochs using Adam (LR=1e-3), decay at epochs 30 and 60. Loss weights are $\alpha=\beta=\gamma=0.5$.

\subsection{Results}
On COCO, our student achieves 67.5\% AP vs. 65.0\% for Lite-HRNet and 62.3\% for TinyPose (Table~\ref{tab:quant}). On MPII, PCKh@0.5 is 90.2\% vs. 89.1\% for Lite-HRNet. Segmentation mIoU is 45.2\%. The student has 2.8M params, compared to 3.4M for Lite-HRNet and 2.0M for TinyPose.

\begin{table}[h]
\centering
\begin{tabular}{lccc}
\hline
Method & COCO AP & MPII PCKh & Params (M) \\
\hline
Lite-HRNet & 65.0 & 89.1 & 3.4 \\
TinyPose    & 62.3 & 87.5 & 2.0 \\
Ours        & \textbf{67.5} & \textbf{90.2} & \textbf{2.8} \\
\hline
\end{tabular}
\caption{Quantitative comparison on COCO and MPII.}
\label{tab:quant}
\end{table}

\end{document}

